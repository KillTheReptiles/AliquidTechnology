{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient(\"mongodb+srv://Admin:admin@aliquid-dbxp3.azure.mongodb.net/Aliquid?retryWrites=true&w=majority\")\n",
    "db = client.Aliquid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 asdad {1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "l = {1,2,3}\n",
    "print(len(l),\"asdad\",l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = db[\"gaming\"]\n",
    "\n",
    "myquery = { \"Ram\": \"\" }\n",
    "\n",
    "mydoc = col.find(myquery)\n",
    "\n",
    "for x in mydoc:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Duvan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Duvan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Duvan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import random\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.blank('es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adios',\n",
       " 'chao',\n",
       " 'hasta',\n",
       " 'luego',\n",
       " 'bye',\n",
       " 'Hola',\n",
       " 'buenos',\n",
       " 'dias',\n",
       " 'holi',\n",
       " 'De',\n",
       " 'que',\n",
       " 'precio',\n",
       " 'hay',\n",
       " 'barato',\n",
       " 'caro',\n",
       " 'gaming',\n",
       " 'juegos',\n",
       " 'jugar',\n",
       " 'gamer',\n",
       " 'ultrabook',\n",
       " 'grande',\n",
       " 'amplio',\n",
       " 'notebook',\n",
       " 'ligero',\n",
       " 'necesito',\n",
       " 'un',\n",
       " 'computador',\n",
       " 'para',\n",
       " 'trabajar',\n",
       " 'trabajar',\n",
       " 'laborar',\n",
       " 'trabajo',\n",
       " '2',\n",
       " 'en',\n",
       " '1',\n",
       " 'desmontable',\n",
       " 'convertible',\n",
       " 'dos',\n",
       " 'en',\n",
       " 'uno',\n",
       " 'tablet',\n",
       " 'netbook',\n",
       " 'pequeño',\n",
       " 'mini',\n",
       " 'minilaptop']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "\n",
    "stop_words = set(stopwords.words(\"spanish\"))\n",
    "\n",
    " \n",
    "colPQRS = db[\"preguntasNuevas\"]\n",
    "\n",
    "\n",
    "for pregunta in colPQRS.find():\n",
    "    for patron in pregunta['preguntas']:\n",
    "\n",
    "\n",
    "\n",
    "        # take each word and tokenize it\n",
    "        w = nltk.word_tokenize(patron)\n",
    "        words.extend(w)\n",
    "        # adding documents\n",
    "\n",
    "words\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet Lemmatizer\n",
      "trouble\n",
      "trouble\n",
      "trouble\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Lematización\n",
    "print(\"WordNet Lemmatizer\")\n",
    "print(lemmatizer.lemmatize(\"trouble\", wordnet.NOUN))\n",
    "print(lemmatizer.lemmatize(\"troubling\", wordnet.VERB))\n",
    "print(lemmatizer.lemmatize(\"troubled\", wordnet.VERB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
